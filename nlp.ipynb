{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c012038b",
   "metadata": {},
   "source": [
    "# Preprocessing Data for NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b7833579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2092dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of data\n",
    "# write and store\n",
    "data = pd.read_csv(\"data\\emails.csv\")\n",
    "data = data[['text', 'spam']]\n",
    "data = data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a4e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text spam\n",
      "0     Subject: naturally irresistible your corporate...    1\n",
      "1     Subject: the stock trading gunslinger  fanny i...    1\n",
      "2     Subject: unbelievable new homes made easy  im ...    1\n",
      "3     Subject: 4 color printing special  request add...    1\n",
      "4     Subject: do not have money , get software cds ...    1\n",
      "...                                                 ...  ...\n",
      "5725  Subject: re : research and development charges...    0\n",
      "5726  Subject: re : receipts from visit  jim ,  than...    0\n",
      "5727  Subject: re : enron case study update  wow ! a...    0\n",
      "5728  Subject: re : interest  david ,  please , call...    0\n",
      "5729  Subject: news : aurora 5 . 2 update  aurora ve...    0\n",
      "\n",
      "[5728 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd95b3d",
   "metadata": {},
   "source": [
    "### Tokenisation of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf80dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text spam\n",
      "0     [Subject, naturally, irresistible, your, corpo...    1\n",
      "1     [Subject, the, stock, trading, gunslinger, fan...    1\n",
      "2     [Subject, unbelievable, new, homes, made, easy...    1\n",
      "3     [Subject, 4, color, printing, special, request...    1\n",
      "4     [Subject, do, not, have, money, get, software,...    1\n",
      "...                                                 ...  ...\n",
      "5725  [Subject, re, research, and, development, char...    0\n",
      "5726  [Subject, re, receipts, from, visit, jim, than...    0\n",
      "5727  [Subject, re, enron, case, study, update, wow,...    0\n",
      "5728  [Subject, re, interest, david, please, call, s...    0\n",
      "5729  [Subject, news, aurora, 5, 2, update, aurora, ...    0\n",
      "\n",
      "[5728 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def tokenisation(data):\n",
    "    data['text'] = data['text'].apply(lambda words: re.sub(r'[^A-Za-z0-9\\s]', '', words))\n",
    "    data['text'] = data['text'].fillna('').astype(str).str.split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6861d98b",
   "metadata": {},
   "source": [
    "### Testing of Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d1593f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb3fa286",
   "metadata": {},
   "source": [
    "### Remove Stopping Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "772b0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "stop_words = {\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \n",
    "    \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \n",
    "    \"between\", \"both\", \"but\", \"by\", \"can\", \"did\", \"do\", \"does\", \"doing\", \"down\", \n",
    "    \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \n",
    "    \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \n",
    "    \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"just\", \"me\", \n",
    "    \"more\", \"most\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"now\", \"of\", \"off\", \n",
    "    \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \n",
    "    \"over\", \"own\", \"same\", \"she\", \"should\", \"so\", \"some\", \"such\", \"than\", \n",
    "    \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \n",
    "    \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \n",
    "    \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"you\", \"your\", \"yours\", \n",
    "    \"yourself\", \"yourselves\", \"subject\", \"re\", \"fw\", \"fwd\", \"http\", \"https\", \"www\", \"com\", \"email\", \n",
    "    \"click\", \"please\", \"thank\", \"thanks\"}\n",
    "\n",
    "def remove_stopwords(data,stop_words):\n",
    "    data['text'] = data['text'].apply(lambda words: [w for w in words if w.lower() not in stop_words])\n",
    "    return data\n",
    "\n",
    "data = remove_stopwords(data,stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950e311",
   "metadata": {},
   "source": [
    "### Testing of Stopping Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf215903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47031dc4",
   "metadata": {},
   "source": [
    "### Stemming of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "521a97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PoterStem(word):\n",
    "    # Step 1a: Handle plurals\n",
    "    if word.endswith(\"sses\"):\n",
    "        word = word[:-4] + \"ss\"\n",
    "    elif word.endswith(\"ies\"):\n",
    "        word = word[:-3] + \"i\"\n",
    "    elif word.endswith(\"s\") and not word.endswith(\"ss\"):\n",
    "        word = word[:-1]\n",
    "    \n",
    "    # Step 1b: Handle past tense\n",
    "    if word.endswith(\"eed\"):\n",
    "        if measure(word[:-3]) > 0:\n",
    "            word = word[:-3] + \"ee\"\n",
    "    elif word.endswith(\"ed\"):\n",
    "        stem = word[:-2]\n",
    "        if has_vowel(stem):\n",
    "            word = stem\n",
    "            # Apply additional rules\n",
    "            if word.endswith(\"at\") or word.endswith(\"bl\") or word.endswith(\"iz\"):\n",
    "                word = word + \"e\"\n",
    "            elif word[-2:] in [\"ll\", \"ss\", \"zz\"] and measure(word) > 1:\n",
    "                word = word[:-1]\n",
    "            elif measure(word) == 1 and cvc(word):\n",
    "                word = word + \"e\"\n",
    "    elif word.endswith(\"ing\"):\n",
    "        stem = word[:-3]  # Fixed: 'ing' is 3 characters\n",
    "        if has_vowel(stem):\n",
    "            word = stem\n",
    "            # Apply additional rules\n",
    "            if word.endswith(\"at\") or word.endswith(\"bl\") or word.endswith(\"iz\"):\n",
    "                word = word + \"e\"\n",
    "            elif word[-2:] in [\"ll\", \"ss\", \"zz\"] and measure(word) > 1:\n",
    "                word = word[:-1]\n",
    "            elif measure(word) == 1 and cvc(word):\n",
    "                word = word + \"e\"\n",
    "    \n",
    "    # Step 1c: Handle 'y'\n",
    "    if word.endswith(\"y\") and has_vowel(word[:-1]):\n",
    "        word = word[:-1] + \"i\"\n",
    "    \n",
    "    # Step 2: Replace suffixes\n",
    "    step2_dict = {\n",
    "        \"ational\": \"ate\",\n",
    "        \"tional\": \"tion\",\n",
    "        \"enci\": \"ence\",\n",
    "        \"anci\": \"ance\",\n",
    "        \"izer\": \"ize\",\n",
    "        \"bli\": \"ble\",\n",
    "        \"alli\": \"al\",\n",
    "        \"entli\": \"ent\",\n",
    "        \"eli\": \"e\",\n",
    "        \"ousli\": \"ous\",\n",
    "        \"ization\": \"ize\",\n",
    "        \"ation\": \"ate\",\n",
    "        \"ator\": \"ate\",\n",
    "        \"alism\": \"al\",\n",
    "        \"iveness\": \"ive\",\n",
    "        \"fulness\": \"ful\",\n",
    "        \"ousness\": \"ous\",\n",
    "        \"aliti\": \"al\",\n",
    "        \"iviti\": \"ive\",\n",
    "        \"biliti\": \"ble\",\n",
    "        \"logi\": \"log\"\n",
    "    }\n",
    "\n",
    "    for suffix in step2_dict.keys():\n",
    "        if word.endswith(suffix) and measure(word[:-len(suffix)]) > 0:\n",
    "            word = word[:-len(suffix)] + step2_dict[suffix]\n",
    "            break\n",
    "    \n",
    "    # Step 3: Replace suffixes\n",
    "    step3_dict = {\n",
    "        \"icate\": \"ic\",\n",
    "        \"ative\": \"\",\n",
    "        \"alize\": \"al\",\n",
    "        \"iciti\": \"ic\",\n",
    "        \"ical\": \"ic\",\n",
    "        \"ful\": \"\",\n",
    "        \"ness\": \"\"\n",
    "    }\n",
    "\n",
    "    for suffix in step3_dict.keys():\n",
    "        if word.endswith(suffix) and measure(word[:-len(suffix)]) > 0:\n",
    "            word = word[:-len(suffix)] + step3_dict[suffix]\n",
    "            break\n",
    "\n",
    "    # Step 4: Remove suffixes\n",
    "    step4_dict = {\n",
    "        \"al\": \"\", \"ance\": \"\", \"ence\": \"\", \"er\": \"\", \"ic\": \"\",\n",
    "        \"able\": \"\", \"ible\": \"\", \"ant\": \"\", \"ement\": \"\",\n",
    "        \"ment\": \"\", \"ent\": \"\", \"ou\": \"\", \"ism\": \"\",\n",
    "        \"ate\": \"\", \"iti\": \"\", \"ous\": \"\", \"ive\": \"\", \"ize\": \"\"\n",
    "    }\n",
    "\n",
    "    for suffix in step4_dict.keys():\n",
    "        if word.endswith(suffix) and measure(word[:-len(suffix)]) > 1:\n",
    "            word = word[:-len(suffix)]\n",
    "            break\n",
    "    \n",
    "    # Special case for 'ion'\n",
    "    if word.endswith(\"ion\") and len(word) > 3:\n",
    "        if measure(word[:-3]) > 1 and word[-4] in \"st\":\n",
    "            word = word[:-3]\n",
    "    \n",
    "    # Step 5a: Remove 'e'\n",
    "    if word.endswith(\"e\"):\n",
    "        stem = word[:-1]\n",
    "        if measure(stem) > 1:\n",
    "            word = stem\n",
    "        elif measure(stem) == 1 and not cvc(stem):\n",
    "            word = stem\n",
    "    \n",
    "    # Step 5b: Remove double 'l'\n",
    "    if word.endswith(\"ll\") and measure(word) > 1:\n",
    "        word = word[:-1]\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def measure(word):\n",
    "    \"\"\"Calculate the measure of a word (number of VC sequences)\"\"\"\n",
    "    vowels = \"aeiou\"\n",
    "    m = 0\n",
    "    in_vowel_seq = False\n",
    "\n",
    "    for char in word:\n",
    "        if char in vowels:\n",
    "            in_vowel_seq = True\n",
    "        else:\n",
    "            if in_vowel_seq:\n",
    "                m += 1\n",
    "                in_vowel_seq = False\n",
    "    return m\n",
    "\n",
    "\n",
    "def has_vowel(word):\n",
    "    \"\"\"Check if word contains a vowel\"\"\"\n",
    "    vowel = [\"a\", \"e\", \"i\", \"o\", \"u\"]\n",
    "    for char in word:\n",
    "        if char in vowel:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def cvc(word):\n",
    "    \"\"\"\n",
    "    Returns True if the word ends with a consonant-vowel-consonant sequence,\n",
    "    where the last consonant is NOT w, x, or y.\n",
    "    \"\"\"\n",
    "    if len(word) < 3:\n",
    "        return False\n",
    "\n",
    "    vowels = \"aeiou\"\n",
    "    last_three = word[-3:]\n",
    "\n",
    "    first, second, last = last_three[0], last_three[1], last_three[2]\n",
    "\n",
    "    if (first not in vowels) and (second in vowels) and (last not in vowels) and last not in \"wxy\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def run_porter(data):\n",
    "    data['text'] = data['text'].apply(lambda words: [PoterStem(w) for w in words])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "29377c6c",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "### Testing of Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6302a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1814996",
   "metadata": {},
   "source": [
    "### Lemmantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "08f57dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise(word):\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Irregular verbs\n",
    "    irregular_verbs = {\n",
    "        'was': 'be', 'were': 'be', 'been': 'be', 'being': 'be', 'am': 'be', 'are': 'be', 'is': 'be',\n",
    "        'had': 'have', 'has': 'have', 'having': 'have',\n",
    "        'did': 'do', 'does': 'do', 'doing': 'do', 'done': 'do',\n",
    "        'went': 'go', 'gone': 'go', 'going': 'go', 'goes': 'go',\n",
    "        'said': 'say', 'says': 'say', 'saying': 'say',\n",
    "        'made': 'make', 'makes': 'make', 'making': 'make',\n",
    "        'took': 'take', 'takes': 'take', 'taken': 'take', 'taking': 'take',\n",
    "        'came': 'come', 'comes': 'come', 'coming': 'come',\n",
    "        'saw': 'see', 'seen': 'see', 'sees': 'see', 'seeing': 'see',\n",
    "        'got': 'get', 'gets': 'get', 'gotten': 'get', 'getting': 'get',\n",
    "        'gave': 'give', 'given': 'give', 'gives': 'give', 'giving': 'give',\n",
    "        'found': 'find', 'finds': 'find', 'finding': 'find',\n",
    "        'told': 'tell', 'tells': 'tell', 'telling': 'tell',\n",
    "        'became': 'become', 'becomes': 'become', 'becoming': 'become',\n",
    "        'left': 'leave', 'leaves': 'leave', 'leaving': 'leave',\n",
    "        'felt': 'feel', 'feels': 'feel', 'feeling': 'feel',\n",
    "        'brought': 'bring', 'brings': 'bring', 'bringing': 'bring',\n",
    "        'began': 'begin', 'begun': 'begin', 'begins': 'begin', 'beginning': 'begin',\n",
    "        'kept': 'keep', 'keeps': 'keep', 'keeping': 'keep',\n",
    "        'held': 'hold', 'holds': 'hold', 'holding': 'hold',\n",
    "        'wrote': 'write', 'written': 'write', 'writes': 'write', 'writing': 'write',\n",
    "        'stood': 'stand', 'stands': 'stand', 'standing': 'stand',\n",
    "        'ran': 'run', 'runs': 'run', 'running': 'run',\n",
    "        'bought': 'buy', 'buys': 'buy', 'buying': 'buy',\n",
    "        'spoke': 'speak', 'spoken': 'speak', 'speaks': 'speak', 'speaking': 'speak',\n",
    "    }\n",
    "    \n",
    "    if word in irregular_verbs:\n",
    "        return irregular_verbs[word]\n",
    "    \n",
    "    # Irregular plurals\n",
    "    irregular_plurals = {\n",
    "        'children': 'child', 'men': 'man', 'women': 'woman', 'people': 'person',\n",
    "        'feet': 'foot', 'teeth': 'tooth', 'geese': 'goose', 'mice': 'mouse',\n",
    "        'oxen': 'ox', 'sheep': 'sheep', 'deer': 'deer', 'fish': 'fish',\n",
    "        'lives': 'life', 'wives': 'wife', 'knives': 'knife', 'leaves': 'leaf',\n",
    "    }\n",
    "    \n",
    "    if word in irregular_plurals:\n",
    "        return irregular_plurals[word]\n",
    "    \n",
    "    if word.endswith('ing'):\n",
    "        if len(word) > 5 and word[-4] == word[-5]:\n",
    "            return word[:-4]\n",
    "        return word[:-3]\n",
    "    \n",
    "    if word.endswith('ed'):\n",
    "        if len(word) > 4 and word[-3] == word[-4]:\n",
    "            return word[:-3]\n",
    "        return word[:-2]\n",
    "    \n",
    "    if word.endswith('ies') and len(word) > 4:\n",
    "        return word[:-3] + 'y'\n",
    "    \n",
    "    if word.endswith('es') and len(word) > 3:\n",
    "        if word[-3] in 'sxz' or word[-4:-2] in ['ch', 'sh']:\n",
    "            return word[:-2]\n",
    "    \n",
    "    if word.endswith('s') and len(word) > 2:\n",
    "        return word[:-1]\n",
    "    \n",
    "    if word.endswith('ly') and len(word) > 4:\n",
    "        return word[:-2]\n",
    "    \n",
    "    if word.endswith('er') and len(word) > 4:\n",
    "        if word[-3] == word[-4]:\n",
    "            return word[:-3]\n",
    "        return word[:-2]\n",
    "    \n",
    "    if word.endswith('est') and len(word) > 5:\n",
    "        if word[-4] == word[-5]:\n",
    "            return word[:-4]\n",
    "        return word[:-3]\n",
    "    \n",
    "    return word\n",
    "\n",
    "def run_lemmantisation(data):\n",
    "    data['text'] = data['text'].apply(lambda words: [lemmatise(w) for w in words])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13893b4",
   "metadata": {},
   "source": [
    "### Testing of Lemmantisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "99ef84c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see\n",
      "see\n",
      "come\n",
      "come\n",
      "cool\n",
      "see\n"
     ]
    }
   ],
   "source": [
    "testing_data = [\"seen\", \"saw\",\"came\",\"come\",\"coolest\", \"saw\"]\n",
    "for i in testing_data:\n",
    "    print(lemmatise(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69958cb5",
   "metadata": {},
   "source": [
    "# Writing of NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1445a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data,if_lemma=True):\n",
    "    data = tokenisation(data)  # tokens put in 'text_tokens'\n",
    "    data = data[['text', 'spam']]\n",
    "    data = remove_stopwords(data,stop_words)\n",
    "    if if_lemma:\n",
    "        data = run_lemmantisation(data)\n",
    "    else:\n",
    "        data = run_porter(data)\n",
    "\n",
    "    return data\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "124a7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user():\n",
    "    while True:\n",
    "        choice = input(\"Select 1 for lemmantisation or 2 for stemming: \").strip()\n",
    "        if choice in [\"1\",\"2\"]:\n",
    "            if int(choice) == 1:\n",
    "                choice = True\n",
    "            else:\n",
    "                choice = False\n",
    "            break\n",
    "        else:\n",
    "            print(\"please answer 1 or 2\")\n",
    "    \n",
    "    return choice    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8051f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "def nlp(data,alpha):\n",
    "\n",
    "    # setting params \n",
    "    min_count = 5\n",
    "    ngram = 1\n",
    "    \n",
    "    # preprocess data\n",
    "    choice = ask_user()\n",
    "    dat = preprocessing(data,choice)\n",
    "\n",
    "    count_spam = Counter()\n",
    "    count_ham = Counter()\n",
    "    msg_count_spam = 0\n",
    "    msg_count_ham = 0 \n",
    "\n",
    "    # processing the data\n",
    "    for index, cols in dat.iterrows():\n",
    "\n",
    "        tokens = cols['text']\n",
    "        if ngram > 1:\n",
    "            tokens = make_ngrams(tokens,ngram)\n",
    "    \n",
    "        if cols['spam'] == \"1\":\n",
    "            msg_count_spam += 1\n",
    "            count_spam.update(tokens)\n",
    "        else:\n",
    "            msg_count_ham += 1\n",
    "            count_ham.update(tokens)\n",
    "\n",
    "    # calculate totals and prior proabilities\n",
    "    total_tokens_spam = sum(count_spam.values())\n",
    "    total_tokens_ham = sum(count_ham.values())\n",
    "\n",
    "    vocab = set(count_spam.keys()) | set(count_spam.keys())\n",
    "    V = len(vocab)\n",
    "\n",
    "    P_spam_prior = msg_count_spam / (msg_count_spam + msg_count_ham)\n",
    "\n",
    "    # building the dictionary\n",
    "    dictionary = {}\n",
    "    for w in vocab:\n",
    "        Cs = count_spam[w]\n",
    "        Ch = count_ham[w]\n",
    "\n",
    "        p_w_given_spam = (Cs + alpha) / (total_tokens_spam + alpha * V)\n",
    "        p_w_given_ham = (Ch + alpha) / (total_tokens_ham + alpha * V)\n",
    "\n",
    "        p_spam_given_w = (p_w_given_spam * P_spam_prior) / \\\n",
    "                     (p_w_given_spam * P_spam_prior + p_w_given_ham * (1 - P_spam_prior))\n",
    "        \n",
    "\n",
    "        total_count = Cs + Ch\n",
    "        if total_count >= min_count:\n",
    "            dictionary[w] = {\n",
    "                \"count_spam\": Cs,\n",
    "                \"count_ham\": Ch,\n",
    "                \"p_spam\": round(p_spam_given_w, 6)\n",
    "            }\n",
    "    \n",
    "    # Step 4: Export\n",
    "    with open(\"spam_dictionary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dictionary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "\n",
    "def make_ngrams(tokens, n=1):\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = '_'.join(tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nlp(data,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd2df1",
   "metadata": {},
   "source": [
    "### Run Tests against Spam Words Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read and clean the data\n",
    "data = pd.read_csv(r\"data\\emails.csv\")\n",
    "data = data[['text', 'spam']]\n",
    "data = data.dropna()\n",
    "\n",
    "# Get the first row as a dictionary\n",
    "text_val_data = data.loc[1].to_dict()\n",
    "\n",
    "# Create a new DataFrame from that dictionary\n",
    "text_val = pd.DataFrame({\n",
    "    \"text\": [text_val_data['text']],\n",
    "    \"spam\": [text_val_data['spam']]\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "287d3630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT SPAM\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def model(fit):\n",
    "    with open(\"spam_dictionary.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        dictionary = json.load(file)\n",
    "\n",
    "    list_of_probs = []\n",
    "    test_val = preprocessing(fit)\n",
    "\n",
    "    for word in test_val['text']:\n",
    "        for w in word:\n",
    "            try:\n",
    "                list_of_probs.append(dictionary[w]['p_spam'])\n",
    "            except KeyError:\n",
    "                continue\n",
    "    \n",
    "    prop_val_mean = sum(list_of_probs)/len(list_of_probs)\n",
    "    if prop_val_mean > 0.5:\n",
    "        return \"SPAM\"\n",
    "    else:\n",
    "        return \"NOT SPAM\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(model(text_val))\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
